# Train Ret-XKnow model on WebQA dataset
experiment:
    exp_name: "InBatch"
    description: "xknow-okvqa_wiki-ft"
    path_suffix: "xknow/okvqa_wiki/ft"

# WandB settings
wandb_config:
    enabled: True
    experiment_name: "${experiment.description}"

# Logger settings
logger_config:
    logger_out_dir: "logger/${experiment.path_suffix}"  # logger will be saved to mbeir_dir/logger/experiment.path_suffix
    logger_out_file_name: "train.log"  #TODO: add date to log file name

# Dataset settings
data_config:
    image_size: 224, 224
    hard_neg_num: 0
    in_batch_neg_num: 0  # TODO: Move this to model config
    shuffle_cand: True
    returns: null
    dataset_name: "okvqa"
    okvqa:
        data_path: data/okvqa/train2014_pairs_cap_combine_sum.txt
        valid_data_path: data/okvqa/val2014_pairs_cap_combine_sum.txt
        img_dir: data/images
        image_cached: False


# DataLoader settings
dataloader_config:
    num_workers: 0
    train_batch_size: 64
    valid_batch_size: 64

# Trainer settings
trainer_config:
    gradient_accumulation_steps: 1
    num_train_epochs: 10
    learning_rate: 2e-5
    warmup_steps: 200
    eval_steps: 500
    early_stop: 5  # TODO: we are not using this.
    print_freq: 50
    weight_decay: 0.0 
    frozen: False
    pretrained_checkpoint: "ckpts/xknow/vid2r/base/xknow_pretrained.pth"

# Evaluator settings
evaluator:
    enable_eval: True
    eval_freq: 1
    print_freq: 10

# Model settings
model:
    short_name: "xknow"
    vision_model_name: "openai/clip-vit-base-patch32"
    colbert_checkpoint: "ckpts/colbertv2.0"
    gather_embeddings: True
    num_tokens: 32
    hidden_size: 768
    pretraining: False
    kernel_size: 3

ckpt_config:
    ckpt_dir: "ckpts/${experiment.path_suffix}" # ckpt will be saved to mbeir_dir/mbeir_checkpoint/experiment.path_suffix
    resume_training: False
    ckpt_name: ""

# Random seed
seed: 2024

# Distributed training settings
dist_config:
    dist_url: "env://"