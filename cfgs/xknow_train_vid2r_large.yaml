# Train Ret-XKnow model on WebQA dataset
experiment:
    exp_name: "InBatch"
    description: "xknow-vid2r-large"
    path_suffix: "xknow/vid2r/large"

# WandB settings
wandb_config:
    enabled: True
    experiment_name: "${experiment.description}"

# Logger settings
logger_config:
    logger_out_dir: "logger/${experiment.path_suffix}"  # logger will be saved to mbeir_dir/logger/experiment.path_suffix
    logger_out_file_name: "train.log"  #TODO: add date to log file name

# Dataset settings
data_config:
    image_size: 224, 224
    hard_neg_num: 0
    in_batch_neg_num: 0  # TODO: Move this to model config
    shuffle_cand: True
    returns: null
    nways: 1
    dataset_name: "vid2r"
    vid2r:
        data_path: data/vid2r/ViD2R.json
        img_dir: data/vid2r/images
        image_cached: True
        valid_samples: 1024


# DataLoader settings
dataloader_config:
    num_workers: 0
    train_batch_size: 64  # 78597MiB / 81559MiB
    valid_batch_size: 64

# Trainer settings
trainer_config:
    gradient_accumulation_steps: 1
    num_train_epochs: 15
    learning_rate: 5e-5
    warmup_steps: 100
    eval_steps: 500
    early_stop: 5  # TODO: we are not using this.
    print_freq: 50
    weight_decay: 0.0 # 0.01 #TODO: we are not using this.
    pretrained_checkpoint: ""
    frozen: True

# Evaluator settings
evaluator:
    enable_eval: True
    eval_freq: 1
    print_freq: 10

# Model settings
model:
    short_name: "xknow"
    vision_model_name: "openai/clip-vit-large-patch14"
    colbert_checkpoint: "ckpts/colbertv2.0"
    gather_embeddings: True
    num_tokens: 32
    kernel_size: 4
    hidden_size: 768
    pretraining: True

ckpt_config:
    ckpt_dir: "ckpts/${experiment.path_suffix}" # ckpt will be saved to mbeir_dir/mbeir_checkpoint/experiment.path_suffix
    resume_training: False
    ckpt_name: ""

# Random seed
seed: 2024

# Distributed training settings
dist_config:
    dist_url: "env://"